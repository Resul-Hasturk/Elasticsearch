[2020-12-07T09:20:21,267][WARN ][o.e.c.r.a.DiskThresholdMonitor] [master-3] flood stage disk watermark [20gb] exceeded on [hcPCtDaERsWlwXUdl5vmiw][data-11][/var/lib/elasticsearch/nodes/0] free: 2.3gb[48.3%], all indices on this node will be marked read-only
[2020-12-07T09:20:26,442][ERROR][o.e.x.m.c.i.IndexStatsCollector] [master-3] collector [index-stats] timed out when collecting data
[2020-12-07T09:20:51,281][WARN ][o.e.c.r.a.DiskThresholdMonitor] [master-3] flood stage disk watermark [20gb] exceeded on [hcPCtDaERsWlwXUdl5vmiw][data-11][/var/lib/elasticsearch/nodes/0] free: 2.3gb[48.3%], all indices on this node will be marked read-only
[2020-12-07T09:21:22,241][WARN ][o.e.c.r.a.DiskThresholdMonitor] [master-3] flood stage disk watermark [20gb] exceeded on [hcPCtDaERsWlwXUdl5vmiw][data-11][/var/lib/elasticsearch/nodes/0] free: 2.3gb[48.3%], all indices on this node will be marked read-only
[2020-12-07T09:21:53,060][WARN ][o.e.c.r.a.DiskThresholdMonitor] [master-3] flood stage disk watermark [20gb] exceeded on [hcPCtDaERsWlwXUdl5vmiw][data-11][/var/lib/elasticsearch/nodes/0] free: 2.3gb[48.3%], all indices on this node will be marked read-only
[2020-12-07T09:22:24,672][WARN ][o.e.c.r.a.DiskThresholdMonitor] [master-3] flood stage disk watermark [20gb] exceeded on [hcPCtDaERsWlwXUdl5vmiw][data-11][/var/lib/elasticsearch/nodes/0] free: 2.3gb[48.3%], all indices on this node will be marked read-only
[2020-12-07T09:22:47,189][WARN ][o.e.t.TransportService   ] [master-3] Received response for a request that has timed out, sent [12807ms] ago, timed out [2802ms] ago, action [internal:coordination/fault_detection/follower_check], node [{data-1}{_BUmAFaYTGi-6aaYnPlqvw}{R8bdK4JlRVK7CpqNOk4Qeg}{192.168.3.68}{192.168.3.68:9300}{dil}{ml.machine_memory=270073581568, ml.max_open_jobs=20, xpack.installed=true, disk_type=ssd}], id [331367427]
[2020-12-07T09:23:05,635][WARN ][o.e.t.TransportService   ] [master-3] Received response for a request that has timed out, sent [17609ms] ago, timed out [7604ms] ago, action [internal:coordination/fault_detection/follower_check], node [{data-1}{_BUmAFaYTGi-6aaYnPlqvw}{R8bdK4JlRVK7CpqNOk4Qeg}{192.168.3.68}{192.168.3.68:9300}{dil}{ml.machine_memory=270073581568, ml.max_open_jobs=20, xpack.installed=true, disk_type=ssd}], id [331367651]
[2020-12-07T09:23:09,673][DEBUG][o.e.a.a.c.n.s.TransportNodesStatsAction] [master-3] failed to execute on node [_BUmAFaYTGi-6aaYnPlqvw]
org.elasticsearch.transport.ReceiveTimeoutTransportException: [data-1][192.168.3.68:9300][cluster:monitor/nodes/stats[n]] request_id [331367733] timed out after [15008ms]
        at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:1022) [elasticsearch-7.4.2.jar:7.4.2]
        at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:703) [elasticsearch-7.4.2.jar:7.4.2]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
        at java.lang.Thread.run(Thread.java:830) [?:?]
[2020-12-07T09:23:24,674][WARN ][o.e.c.InternalClusterInfoService] [master-3] Failed to update shard information for ClusterInfoUpdateJob within 15s timeout
[2020-12-07T09:23:24,675][WARN ][o.e.c.r.a.DiskThresholdMonitor] [master-3] flood stage disk watermark [20gb] exceeded on [hcPCtDaERsWlwXUdl5vmiw][data-11][/var/lib/elasticsearch/nodes/0] free: 2.3gb[48.3%], all indices on this node will be marked read-only
[2020-12-07T09:23:39,784][WARN ][o.e.t.TransportService   ] [master-3] Received response for a request that has timed out, sent [22012ms] ago, timed out [12006ms] ago, action [internal:coordination/fault_detection/follower_check], node [{data-1}{_BUmAFaYTGi-6aaYnPlqvw}{R8bdK4JlRVK7CpqNOk4Qeg}{192.168.3.68}{192.168.3.68:9300}{dil}{ml.machine_memory=270073581568, ml.max_open_jobs=20, xpack.installed=true, disk_type=ssd}], id [331368118]
[2020-12-07T09:23:39,784][WARN ][o.e.t.TransportService   ] [master-3] Received response for a request that has timed out, sent [11006ms] ago, timed out [1001ms] ago, action [internal:coordination/fault_detection/follower_check], node [{data-1}{_BUmAFaYTGi-6aaYnPlqvw}{R8bdK4JlRVK7CpqNOk4Qeg}{192.168.3.68}{192.168.3.68:9300}{dil}{ml.machine_memory=270073581568, ml.max_open_jobs=20, xpack.installed=true, disk_type=ssd}], id [331368274]
[2020-12-07T09:24:09,677][DEBUG][o.e.a.a.c.n.s.TransportNodesStatsAction] [master-3] failed to execute on node [_BUmAFaYTGi-6aaYnPlqvw]
org.elasticsearch.transport.ReceiveTimeoutTransportException: [data-1][192.168.3.68:9300][cluster:monitor/nodes/stats[n]] request_id [331368655] timed out after [15008ms]
        at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:1022) [elasticsearch-7.4.2.jar:7.4.2]
        at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:703) [elasticsearch-7.4.2.jar:7.4.2]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
        at java.lang.Thread.run(Thread.java:830) [?:?]
[2020-12-07T09:24:12,904][INFO ][o.e.c.s.MasterService    ] [master-3] node-left[{data-1}{_BUmAFaYTGi-6aaYnPlqvw}{R8bdK4JlRVK7CpqNOk4Qeg}{192.168.3.68}{192.168.3.68:9300}{dil}{ml.machine_memory=270073581568, ml.max_open_jobs=20, xpack.installed=true, disk_type=ssd} followers check retry count exceeded], term: 13, version: 722362, reason: removed {{data-1}{_BUmAFaYTGi-6aaYnPlqvw}{R8bdK4JlRVK7CpqNOk4Qeg}{192.168.3.68}{192.168.3.68:9300}{dil}{ml.machine_memory=270073581568, ml.max_open_jobs=20, xpack.installed=true, disk_type=ssd},}
[2020-12-07T09:24:14,643][INFO ][o.e.c.s.ClusterApplierService] [master-3] removed {{data-1}{_BUmAFaYTGi-6aaYnPlqvw}{R8bdK4JlRVK7CpqNOk4Qeg}{192.168.3.68}{192.168.3.68:9300}{dil}{ml.machine_memory=270073581568, ml.max_open_jobs=20, xpack.installed=true, disk_type=ssd},}, term: 13, version: 722362, reason: Publication{term=13, version=722362}
[2020-12-07T09:24:14,645][DEBUG][o.e.a.a.c.n.i.TransportNodesInfoAction] [master-3] failed to execute on node [_BUmAFaYTGi-6aaYnPlqvw]
org.elasticsearch.transport.NodeDisconnectedException: [data-1][192.168.3.68:9300][cluster:monitor/nodes/info[n]] disconnected
[2020-12-07T09:24:14,649][INFO ][o.e.c.r.DelayedAllocationService] [master-3] scheduling reroute for delayed shards in [58.1s] (59 delayed shards)
[2020-12-07T09:24:14,655][INFO ][o.e.c.m.MetaDataIndexTemplateService] [master-3] adding template [.management-beats] for index patterns [.management-beats]
[2020-12-07T09:24:14,655][DEBUG][o.e.a.a.c.n.i.TransportNodesInfoAction] [master-3] failed to execute on node [_BUmAFaYTGi-6aaYnPlqvw]
org.elasticsearch.transport.NodeDisconnectedException: [data-1][192.168.3.68:9300][cluster:monitor/nodes/info[n]] disconnected
[2020-12-07T09:24:14,664][DEBUG][o.e.a.a.i.s.TransportIndicesStatsAction] [master-3] failed to execute [indices:monitor/stats] on node [_BUmAFaYTGi-6aaYnPlqvw]
org.elasticsearch.transport.NodeDisconnectedException: [data-1][192.168.3.68:9300][indices:monitor/stats[n]] disconnected
[2020-12-07T09:24:14,666][WARN ][o.e.c.r.a.DiskThresholdMonitor] [master-3] flood stage disk watermark [20gb] exceeded on [hcPCtDaERsWlwXUdl5vmiw][data-11][/var/lib/elasticsearch/nodes/0] free: 2.3gb[48.3%], all indices on this node will be marked read-only
[2020-12-07T09:24:14,666][DEBUG][o.e.a.a.c.n.i.TransportNodesInfoAction] [master-3] failed to execute on node [_BUmAFaYTGi-6aaYnPlqvw]
org.elasticsearch.transport.NodeDisconnectedException: [data-1][192.168.3.68:9300][cluster:monitor/nodes/info[n]] disconnected
[2020-12-07T09:24:14,676][DEBUG][o.e.a.a.c.n.i.TransportNodesInfoAction] [master-3] failed to execute on node [_BUmAFaYTGi-6aaYnPlqvw]
org.elasticsearch.transport.NodeDisconnectedException: [data-1][192.168.3.68:9300][cluster:monitor/nodes/info[n]] disconnected
[2020-12-07T09:24:14,685][DEBUG][o.e.a.a.i.s.TransportIndicesStatsAction] [master-3] failed to execute [indices:monitor/stats] on node [_BUmAFaYTGi-6aaYnPlqvw]
org.elasticsearch.transport.NodeDisconnectedException: [data-1][192.168.3.68:9300][indices:monitor/stats[n]] disconnected
[2020-12-07T09:24:14,720][INFO ][o.e.c.m.MetaDataIndexTemplateService] [master-3] adding template [.management-beats] for index patterns [.management-beats]
[2020-12-07T09:24:14,800][INFO ][o.e.c.m.MetaDataIndexTemplateService] [master-3] adding template [.management-beats] for index patterns [.management-beats]
[2020-12-07T09:24:15,139][WARN ][o.e.c.r.a.AllocationService] [master-3] [company][5] marking unavailable shards as stale: [kO_ofnNPRRi4_N4RhWHazg]
[2020-12-07T09:24:15,139][WARN ][o.e.c.r.a.AllocationService] [master-3] [company][8] marking unavailable shards as stale: [NdEsmf6lQA63hmui9wJMJw]
[2020-12-07T09:24:15,139][WARN ][o.e.c.r.a.AllocationService] [master-3] [ticket][4] marking unavailable shards as stale: [pESYx4WUQ1uEfeKFZz5MBQ]
[2020-12-07T09:24:15,139][WARN ][o.e.c.r.a.AllocationService] [master-3] [moving][0] marking unavailable shards as stale: [fBYt-JuVS4yLcuSK2x2DXA]
[2020-12-07T09:24:15,139][WARN ][o.e.c.r.a.AllocationService] [master-3] [sales][28] marking unavailable shards as stale: [RZPGRdb_QOmNOytb9Cx6xw]
[2020-12-07T09:24:15,139][WARN ][o.e.c.r.a.AllocationService] [master-3] [sales][29] marking unavailable shards as stale: [grc618VHTaCxvNbVVwJNVw]
[2020-12-07T09:24:15,139][WARN ][o.e.c.r.a.AllocationService] [master-3] [sales][25] marking unavailable shards as stale: [MXL3faG_TyOdOCXDRSHUIw]
[2020-12-07T09:24:15,139][WARN ][o.e.c.r.a.AllocationService] [master-3] [sales][18] marking unavailable shards as stale: [1sfV3wwCTKalj2SBI32zcQ]
[2020-12-07T09:24:15,139][WARN ][o.e.c.r.a.AllocationService] [master-3] [sales][14] marking unavailable shards as stale: [jcx7jMCWTKmZLjd9pq1aGg]
[2020-12-07T09:24:15,140][WARN ][o.e.c.r.a.AllocationService] [master-3] [sales][15] marking unavailable shards as stale: [jHngzxqjRCO87cuyyCTXqg]
[2020-12-07T09:24:15,140][WARN ][o.e.c.r.a.AllocationService] [master-3] [sales][16] marking unavailable shards as stale: [adpN4wH4RD2xZ99Jd_19JQ]
[2020-12-07T09:24:15,140][WARN ][o.e.c.r.a.AllocationService] [master-3] [sales][10] marking unavailable shards as stale: [EFioOlq5QaSX6qjOm4V1eA]
[2020-12-07T09:24:15,140][WARN ][o.e.c.r.a.AllocationService] [master-3] [sales][13] marking unavailable shards as stale: [asiqOFIyREO0x3ItAWOhLQ]
[2020-12-07T09:24:15,140][WARN ][o.e.c.r.a.AllocationService] [master-3] [sales][9] marking unavailable shards as stale: [U3l67PreT7qOZZwYqyE-sw]
[2020-12-07T09:24:15,140][WARN ][o.e.c.r.a.AllocationService] [master-3] [sales][34] marking unavailable shards as stale: [mTbrFdydTzKLupMr98zZIw]
[2020-12-07T09:24:15,140][WARN ][o.e.c.r.a.AllocationService] [master-3] [sales][3] marking unavailable shards as stale: [8FDFoRfwSqWvQGhq3oo6TA]
[2020-12-07T09:24:15,140][WARN ][o.e.c.r.a.AllocationService] [master-3] [sales][30] marking unavailable shards as stale: [gTNv0oi6S7SowzCy2aj0LA]
[2020-12-07T09:24:15,140][WARN ][o.e.c.r.a.AllocationService] [master-3] [sales][33] marking unavailable shards as stale: [nJlVutsgRnK4hIKDrMg7UQ]
[2020-12-07T09:24:15,140][WARN ][o.e.c.r.a.AllocationService] [master-3] [sales][1] marking unavailable shards as stale: [yzwyVJYzQKGNEbo0FTUQ5g]
[2020-12-07T09:24:15,140][WARN ][o.e.c.r.a.AllocationService] [master-3] [worker][5] marking unavailable shards as stale: [npOXLRKkShOH3jEfnMuhRg]
[2020-12-07T09:24:15,140][WARN ][o.e.c.r.a.AllocationService] [master-3] [worker][6] marking unavailable shards as stale: [GMDkCSOjQGmAq1TWyAuGQw]
[2020-12-07T09:24:15,491][WARN ][o.e.c.r.a.AllocationService] [master-3] [sales][4] marking unavailable shards as stale: [kk2-tlF0SDieo5RmzxoAew]
[2020-12-07T09:24:18,313][WARN ][o.e.c.r.a.AllocationService] [master-3] [sales][7] marking unavailable shards as stale: [ePrNU0OIQGWL75klrCBUEA]
[2020-12-07T09:24:20,403][ERROR][o.e.c.a.s.ShardStateAction] [master-3] [sales][4] unexpected failure while failing shard [shard id [[sales][4]], allocation id [78HUYrhjQSapswOeGb3yWw], primary term [20], message [failed to perform indices:data/write/bulk[s] on replica [sales][4], node[vKA6qG2WRr2VJnK4wNVYOA], [R], s[STARTED], a[id=78HUYrhjQSapswOeGb3yWw]], failure [RemoteTransportException[[data-2][192.168.3.69:9300][indices:data/write/bulk[s][r]]]; nested: IllegalStateException[[sales][4] operation primary term [20] is too old (current [21])]; ], markAsStale [true]]
org.elasticsearch.cluster.action.shard.ShardStateAction$NoLongerPrimaryShardException: primary term [20] did not match current primary term [21]
        at org.elasticsearch.cluster.action.shard.ShardStateAction$ShardFailedClusterStateTaskExecutor.execute(ShardStateAction.java:365) ~[elasticsearch-7.4.2.jar:7.4.2]
        at org.elasticsearch.cluster.service.MasterService.executeTasks(MasterService.java:702) ~[elasticsearch-7.4.2.jar:7.4.2]
        at org.elasticsearch.cluster.service.MasterService.calculateTaskOutputs(MasterService.java:324) ~[elasticsearch-7.4.2.jar:7.4.2]
        at org.elasticsearch.cluster.service.MasterService.runTasks(MasterService.java:219) [elasticsearch-7.4.2.jar:7.4.2]
        at org.elasticsearch.cluster.service.MasterService.access$000(MasterService.java:73) [elasticsearch-7.4.2.jar:7.4.2]
        at org.elasticsearch.cluster.service.MasterService$Batcher.run(MasterService.java:151) [elasticsearch-7.4.2.jar:7.4.2]
        at org.elasticsearch.cluster.service.TaskBatcher.runIfNotProcessed(TaskBatcher.java:150) [elasticsearch-7.4.2.jar:7.4.2]
        at org.elasticsearch.cluster.service.TaskBatcher$BatchedTask.run(TaskBatcher.java:188) [elasticsearch-7.4.2.jar:7.4.2]
        at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:703) [elasticsearch-7.4.2.jar:7.4.2]
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:252) [elasticsearch-7.4.2.jar:7.4.2]
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:215) [elasticsearch-7.4.2.jar:7.4.2]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
        at java.lang.Thread.run(Thread.java:830) [?:?]
[2020-12-07T09:24:20,471][ERROR][o.e.c.a.s.ShardStateAction] [master-3] [sales][7] unexpected failure while failing shard [shard id [[sales][7]], allocation id [afwjlurvRZORor5VtFfljg], primary term [9], message [failed to perform indices:data/write/bulk[s] on replica [sales][7], node[NVXXnkg9RcS1LJHbxE4JWw], [R], s[STARTED], a[id=afwjlurvRZORor5VtFfljg]], failure [RemoteTransportException[[data-4][192.168.3.66:9300][indices:data/write/bulk[s][r]]]; nested: IllegalStateException[[sales][7] operation primary term [9] is too old (current [10])]; ], markAsStale [true]]
org.elasticsearch.cluster.action.shard.ShardStateAction$NoLongerPrimaryShardException: primary term [9] did not match current primary term [10]
        at org.elasticsearch.cluster.action.shard.ShardStateAction$ShardFailedClusterStateTaskExecutor.execute(ShardStateAction.java:365) ~[elasticsearch-7.4.2.jar:7.4.2]
        at org.elasticsearch.cluster.service.MasterService.executeTasks(MasterService.java:702) ~[elasticsearch-7.4.2.jar:7.4.2]
        at org.elasticsearch.cluster.service.MasterService.calculateTaskOutputs(MasterService.java:324) ~[elasticsearch-7.4.2.jar:7.4.2]
        at org.elasticsearch.cluster.service.MasterService.runTasks(MasterService.java:219) [elasticsearch-7.4.2.jar:7.4.2]
        at org.elasticsearch.cluster.service.MasterService.access$000(MasterService.java:73) [elasticsearch-7.4.2.jar:7.4.2]
        at org.elasticsearch.cluster.service.MasterService$Batcher.run(MasterService.java:151) [elasticsearch-7.4.2.jar:7.4.2]
        at org.elasticsearch.cluster.service.TaskBatcher.runIfNotProcessed(TaskBatcher.java:150) [elasticsearch-7.4.2.jar:7.4.2]
        at org.elasticsearch.cluster.service.TaskBatcher$BatchedTask.run(TaskBatcher.java:188) [elasticsearch-7.4.2.jar:7.4.2]
        at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:703) [elasticsearch-7.4.2.jar:7.4.2]
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:252) [elasticsearch-7.4.2.jar:7.4.2]
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:215) [elasticsearch-7.4.2.jar:7.4.2]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
        at java.lang.Thread.run(Thread.java:830) [?:?]
[2020-12-07T09:24:20,601][ERROR][o.e.c.a.s.ShardStateAction] [master-3] [sales][4] unexpected failure while failing shard [shard id [[sales][4]], allocation id [78HUYrhjQSapswOeGb3yWw], primary term [20], message [failed to perform indices:data/write/bulk[s] on replica [sales][4], node[vKA6qG2WRr2VJnK4wNVYOA], [R], s[STARTED], a[id=78HUYrhjQSapswOeGb3yWw]], failure [RemoteTransportException[[data-2][192.168.3.69:9300][indices:data/write/bulk[s][r]]]; nested: IllegalStateException[[sales][4] operation primary term [20] is too old (current [21])]; ], markAsStale [true]]
org.elasticsearch.cluster.action.shard.ShardStateAction$NoLongerPrimaryShardException: primary term [20] did not match current primary term [21]
        at org.elasticsearch.cluster.action.shard.ShardStateAction$ShardFailedClusterStateTaskExecutor.execute(ShardStateAction.java:365) ~[elasticsearch-7.4.2.jar:7.4.2]
        at org.elasticsearch.cluster.service.MasterService.executeTasks(MasterService.java:702) ~[elasticsearch-7.4.2.jar:7.4.2]
        at org.elasticsearch.cluster.service.MasterService.calculateTaskOutputs(MasterService.java:324) ~[elasticsearch-7.4.2.jar:7.4.2]
        at org.elasticsearch.cluster.service.MasterService.runTasks(MasterService.java:219) [elasticsearch-7.4.2.jar:7.4.2]
        at org.elasticsearch.cluster.service.MasterService.access$000(MasterService.java:73) [elasticsearch-7.4.2.jar:7.4.2]
        at org.elasticsearch.cluster.service.MasterService$Batcher.run(MasterService.java:151) [elasticsearch-7.4.2.jar:7.4.2]
        at org.elasticsearch.cluster.service.TaskBatcher.runIfNotProcessed(TaskBatcher.java:150) [elasticsearch-7.4.2.jar:7.4.2]
        at org.elasticsearch.cluster.service.TaskBatcher$BatchedTask.run(TaskBatcher.java:188) [elasticsearch-7.4.2.jar:7.4.2]
        at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:703) [elasticsearch-7.4.2.jar:7.4.2]
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:252) [elasticsearch-7.4.2.jar:7.4.2]
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:215) [elasticsearch-7.4.2.jar:7.4.2]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
        at java.lang.Thread.run(Thread.java:830) [?:?]
[2020-12-07T09:24:22,490][INFO ][o.e.c.s.MasterService    ] [master-3] node-join[{data-1}{_BUmAFaYTGi-6aaYnPlqvw}{R8bdK4JlRVK7CpqNOk4Qeg}{192.168.3.68}{192.168.3.68:9300}{dil}{ml.machine_memory=270073581568, ml.max_open_jobs=20, xpack.installed=true, disk_type=ssd} join existing leader], term: 13, version: 722370, reason: added {{data-1}{_BUmAFaYTGi-6aaYnPlqvw}{R8bdK4JlRVK7CpqNOk4Qeg}{192.168.3.68}{192.168.3.68:9300}{dil}{ml.machine_memory=270073581568, ml.max_open_jobs=20, xpack.installed=true, disk_type=ssd},}
[2020-12-07T09:24:32,507][INFO ][o.e.c.c.C.CoordinatorPublication] [master-3] after [10s] publication of cluster state version [722370] is still waiting for {data-1}{_BUmAFaYTGi-6aaYnPlqvw}{R8bdK4JlRVK7CpqNOk4Qeg}{192.168.3.68}{192.168.3.68:9300}{dil}{ml.machine_memory=270073581568, ml.max_open_jobs=20, xpack.installed=true, disk_type=ssd} [SENT_APPLY_COMMIT]
[2020-12-07T09:24:44,974][WARN ][o.e.c.r.a.DiskThresholdMonitor] [master-3] flood stage disk watermark [20gb] exceeded on [hcPCtDaERsWlwXUdl5vmiw][data-11][/var/lib/elasticsearch/nodes/0] free: 2.3gb[48.3%], all indices on this node will be marked read-only
[2020-12-07T09:24:52,507][INFO ][o.e.c.s.ClusterApplierService] [master-3] added {{data-1}{_BUmAFaYTGi-6aaYnPlqvw}{R8bdK4JlRVK7CpqNOk4Qeg}{192.168.3.68}{192.168.3.68:9300}{dil}{ml.machine_memory=270073581568, ml.max_open_jobs=20, xpack.installed=true, disk_type=ssd},}, term: 13, version: 722370, reason: Publication{term=13, version=722370}
[2020-12-07T09:24:52,511][WARN ][o.e.c.c.C.CoordinatorPublication] [master-3] after [30s] publication of cluster state version [722370] is still waiting for {data-1}{_BUmAFaYTGi-6aaYnPlqvw}{R8bdK4JlRVK7CpqNOk4Qeg}{192.168.3.68}{192.168.3.68:9300}{dil}{ml.machine_memory=270073581568, ml.max_open_jobs=20, xpack.installed=true, disk_type=ssd} [SENT_APPLY_COMMIT]
[2020-12-07T09:24:52,626][WARN ][o.e.c.r.a.AllocationService] [master-3] [sales][8] marking unavailable shards as stale: [WB6ePpsOT0qT0ZSEpyeyFg]
[2020-12-07T09:24:52,626][WARN ][o.e.c.r.a.AllocationService] [master-3] [sales][2] marking unavailable shards as stale: [8PcS6QZnS56sJ4Dbrh2-3g]
[2020-12-07T09:24:52,626][WARN ][o.e.c.r.a.AllocationService] [master-3] [sales][0] marking unavailable shards as stale: [2XGjceHNQ5KIB651yFehxQ]
[2020-12-07T09:24:53,256][WARN ][o.e.c.r.a.DiskThresholdMonitor] [master-3] flood stage disk watermark [20gb] exceeded on [hcPCtDaERsWlwXUdl5vmiw][data-11][/var/lib/elasticsearch/nodes/0] free: 2.3gb[48.3%], all indices on this node will be marked read-only
[2020-12-07T09:25:02,741][INFO ][o.e.c.c.C.CoordinatorPublication] [master-3] after [10s] publication of cluster state version [722371] is still waiting for {data-1}{_BUmAFaYTGi-6aaYnPlqvw}{R8bdK4JlRVK7CpqNOk4Qeg}{192.168.3.68}{192.168.3.68:9300}{dil}{ml.machine_memory=270073581568, ml.max_open_jobs=20, xpack.installed=true, disk_type=ssd} [SENT_APPLY_COMMIT]
[2020-12-07T09:25:15,678][WARN ][o.e.c.r.a.DiskThresholdMonitor] [master-3] flood stage disk watermark [20gb] exceeded on [hcPCtDaERsWlwXUdl5vmiw][data-11][/var/lib/elasticsearch/nodes/0] free: 2.3gb[48.3%], all indices on this node will be marked read-only
[2020-12-07T09:25:22,748][WARN ][o.e.c.c.C.CoordinatorPublication] [master-3] after [30s] publication of cluster state version [722371] is still waiting for {data-1}{_BUmAFaYTGi-6aaYnPlqvw}{R8bdK4JlRVK7CpqNOk4Qeg}{192.168.3.68}{192.168.3.68:9300}{dil}{ml.machine_memory=270073581568, ml.max_open_jobs=20, xpack.installed=true, disk_type=ssd} [SENT_APPLY_COMMIT]
[2020-12-07T09:25:32,897][INFO ][o.e.c.c.C.CoordinatorPublication] [master-3] after [10s] publication of cluster state version [722372] is still waiting for {data-1}{_BUmAFaYTGi-6aaYnPlqvw}{R8bdK4JlRVK7CpqNOk4Qeg}{192.168.3.68}{192.168.3.68:9300}{dil}{ml.machine_memory=270073581568, ml.max_open_jobs=20, xpack.installed=true, disk_type=ssd} [SENT_APPLY_COMMIT]
